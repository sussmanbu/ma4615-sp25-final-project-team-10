[
  {
    "objectID": "posts/BlogPost1.html",
    "href": "posts/BlogPost1.html",
    "title": "BlogPost1",
    "section": "",
    "text": "Data Set 1: National Registry of Exonerations (https://www.law.umich.edu/special/exoneration/Pages/detaillist.aspx)\nOne data set we are considering for our project on racial disparities is the National Registry of Exonerations (NRE). This data set (3659 rows, 22 columns) tracks cases of wrongful convictions in the United States and provides detailed information about exonerees, including their race, age, crime type, conviction and exoneration dates, and factors that contributed to their wrongful conviction. The registry, maintained through a collaborative effort involving law schools, innocence organizations, and legal researchers, is designed to shed light on patterns of wrongful convictions and the systemic issues within the justice system. With over 3,000 recorded cases, this data set offers a valuable opportunity to explore how different factors—such as mistaken witness identification, false confessions, or official misconduct—affect exonerees from various racial backgrounds.\nBy analyzing this data, we hope to examine whether racial disparities exist in wrongful convictions and, if so, how they manifest. For instance, do Black exonerees experience official misconduct at a higher rate than white exonerees? Are certain crime types—such as drug offenses or homicides—more likely to lead to wrongful convictions for particular racial groups? Do certain factors—such as official misconduct, inaccurate forensic evidence, or eyewitness misidentification—disproportionately affect exonerees of particular racial groups? Additionally, we want to explore whether exonerees from different racial backgrounds serve significantly different amounts of time before being exonerated. From a practical standpoint, the data comes in a relatively well-structured spreadsheet format (although it is sometimes updated with new cases and additional fields). Loading it into a data analysis environment should be fairly straightforward. However, because this registry is compiled from many sources (court documents, news reports, legal filings, etc.), some challenges with missing or inconsistent data can arise. Despite these limitations, we believe this data set provides a powerful lens through which we can better understand inequities in the criminal justice system and contribute to discussions on how to address them.\nData Set 2: COVID-19 Cases and Deaths by Race/Ethnicity - ARCHIVE (https://data.ct.gov/Health-and-Human-Services/COVID-19-State-Level-Data-Archive/qmgw-5kp6/about_data)\nAnother data set we are considering for our project on racial disparities is the COVID-19 State Level Data - Archive from the Connecticut Department of Public Health (DPH). This dataset tracks state-level COVID-19 metrics and was actively updated until June 1, 2023. It includes 343 rows and 94 columns, offering comprehensive insights into COVID-19 cases, deaths, and testing over time. The data provides cumulative and 7-day rolling metrics, covering confirmed and probable cases, test positivity rates, and reported deaths. The dataset offers an opportunity to investigate racial disparities in COVID-19 outcomes by analyzing statewide trends. Some key questions we hope to explore include: How did COVID-19 case and mortality rates fluctuate over time? Were there significant racial disparities in infection rates and fatalities? How did policy changes, such as testing requirements, impact reported metrics? Additionally, since age-adjusted rates are not directly included, we may need to incorporate external demographic data to better understand disparities.\nFrom a technical perspective, the dataset is well-structured, making it feasible to load into a data analysis environment. However, there are challenges, such as missing data due to delayed reporting and the discontinuation of negative test result tracking after the federal public health emergency ended on May 11, 2023. Despite these limitations, this dataset provides a valuable historical record of state-level COVID-19 impacts and can contribute to broader discussions on health equity and pandemic response policies.\nData Set 3: Drug overdose death rates, by drug type, sex, age, race, and Hispanic origin: United States (https://catalog.data.gov/dataset/drug-overdose-death-rates-by-drug-type-sex-age-race-and-hispanic-origin-united-states-3f72f)\nThe third data set we are considering for our final project is the data on drug overdose death rates, by drug type and selected population characteristics. It includes 15 columns and 6229 rows, which is a large dataset that includes necessary information like gender, race, age to help us have a deeper understand of the trends and patterns surrounding drug overdose fatalities across various demographic groups. By capturing detailed breakdowns by gender, race, and age—along with year-to-year changes—this dataset enables us to pinpoint which subpopulations are most severely affected, how overdose rates have evolved over time, and where public health interventions may be most urgently needed. Some interesting questions we can research on: Are there particular racial or ethnic groups that appear to be disproportionately affected by drug overdoses, and how have these disparities evolved over time? Are there notable interactions between age and gender or age and race in overdose patterns, such as certain age brackets being more susceptible among specific demographic groups?\nFrom a techinical perspective, the dataset is well-organized and contains a rich array of demographic variables, making it feasible to load into most data analysis environments. However, there are challenges, such as potential inconsistencies in data reporting across different regions, limited data coverage for certain demographic groups, and possible variations in drug classification over time. Despite these limitations, this dataset serves as a valuable resource for analyzing patterns and trends in drug overdose fatalities, and it can contribute to broader public health discussions on prevention strategies, resource allocation, and policy-making."
  },
  {
    "objectID": "posts/2025-04-14-Blog-Post-6/BlogPost6.html",
    "href": "posts/2025-04-14-Blog-Post-6/BlogPost6.html",
    "title": "BlogPost6",
    "section": "",
    "text": "library(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(readr)\n\nchicago_crime_data &lt;- read_csv(\"/Users/wmh/Desktop/MA415/Project Team 10/dataset-ignore/Crimes_-_2001_to_Present.csv\")\n\nRows: 8293608 Columns: 22\n\n\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (12): Case Number, Date, Block, IUCR, Primary Type, Description, Locatio...\ndbl  (8): ID, Ward, Community Area, X Coordinate, Y Coordinate, Year, Latitu...\nlgl  (2): Arrest, Domestic\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nchicago_clean &lt;- chicago_crime_data %&gt;%\n  filter(Year &gt;= 2001, !is.na(Year)) %&gt;%\n  filter(`Primary Type` %in% c(\"HOMICIDE\", \"CRIM SEXUAL ASSAULT\", \"ROBBERY\", \"BURGLARY\", \"SEX OFFENSE\")) %&gt;%\n  group_by(Year, `Primary Type`) %&gt;%\n  summarise(Crime_Count = n(), .groups = \"drop\")\nchicago_clean\n\n# A tibble: 120 × 3\n    Year `Primary Type`      Crime_Count\n   &lt;dbl&gt; &lt;chr&gt;                     &lt;int&gt;\n 1  2001 BURGLARY                  26014\n 2  2001 CRIM SEXUAL ASSAULT        1774\n 3  2001 HOMICIDE                    667\n 4  2001 ROBBERY                   18441\n 5  2001 SEX OFFENSE                2239\n 6  2002 BURGLARY                  25623\n 7  2002 CRIM SEXUAL ASSAULT        1802\n 8  2002 HOMICIDE                    658\n 9  2002 ROBBERY                   18523\n10  2002 SEX OFFENSE                2173\n# ℹ 110 more rows\n\n\nSince the Chicago dataset is very huge, to prepare the Chicago crime dataset for comparison with the exoneration data, we first filtered the full dataset only include records from 2001 onward, aligning with the time span of our analysis. We further narrowed the dataset to focus on five major crime categories relevant to potential wrongful convictions: HOMICIDE, CRIM SEXUAL ASSAULT, ROBBERY, BURGLARY, and SEX OFFENSE. These categories were selected based on their severity and overlap with crimes most often associated with exonerations. After filtering, we grouped the data by year and crime type, and counted the number of reported incidents per group. This resulted in a clean, aggregated table of 120 rows (24 years × 5 crime types), which provides a structured yearly summary of serious criminal activity in Chicago. This cleaned data will enable meaningful comparison with the exoneration dataset by aligning both sources on a common yearly time frame and crime category structure.\n\nexoneration_data &lt;- read_csv(\"/Users/wmh/Desktop/MA415/Project Team 10/dataset/publicspreadsheet.csv\")\n\nRows: 3658 Columns: 22\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (18): Last Name, First Name, Race, Sex, State, County, Tags, Worst Crime...\ndbl  (3): Age, Convicted, Exonerated\nnum  (1): Date of Crime Year\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nexon_clean &lt;- exoneration_data %&gt;%\n  filter(County == \"Cook\", !is.na(Convicted)) %&gt;%\n  filter(Convicted &gt;= 2001)\n\nexon_yearly &lt;- exon_clean %&gt;%\n  group_by(Year = Convicted, `Worst Crime Display`) %&gt;%\n  summarise(Exoneration_Count = n(), .groups = \"drop\")\n\ncrime_mapping &lt;- tibble(\n  `Worst Crime Display` = c(\"Murder\", \"Sexual Assault\", \"Robbery\", \"Theft\", \"Sex Offense\"),\n  `Primary Type` = c(\"HOMICIDE\", \"CRIM SEXUAL ASSAULT\", \"ROBBERY\", \"BURGLARY\", \"SEX OFFENSE\")\n)\n\nexon_mapped &lt;- exon_yearly %&gt;%\n  inner_join(crime_mapping, by = \"Worst Crime Display\")\ncombined_panel &lt;- chicago_clean %&gt;%\n  inner_join(exon_mapped, by = c(\"Year\", \"Primary Type\"))\n\ncombined_panel\n\n# A tibble: 20 × 5\n    Year `Primary Type`      Crime_Count `Worst Crime Display` Exoneration_Count\n   &lt;dbl&gt; &lt;chr&gt;                     &lt;int&gt; &lt;chr&gt;                             &lt;int&gt;\n 1  2001 HOMICIDE                    667 Murder                                7\n 2  2002 BURGLARY                  25623 Theft                                 1\n 3  2002 HOMICIDE                    658 Murder                                4\n 4  2003 HOMICIDE                    604 Murder                                2\n 5  2004 CRIM SEXUAL ASSAULT        1527 Sexual Assault                        1\n 6  2004 HOMICIDE                    455 Murder                                3\n 7  2005 HOMICIDE                    453 Murder                                4\n 8  2006 HOMICIDE                    478 Murder                                5\n 9  2007 HOMICIDE                    448 Murder                                2\n10  2009 HOMICIDE                    461 Murder                                2\n11  2011 HOMICIDE                    438 Murder                                4\n12  2012 HOMICIDE                    515 Murder                                1\n13  2012 ROBBERY                   13484 Robbery                               1\n14  2013 BURGLARY                  17894 Theft                                 1\n15  2014 HOMICIDE                    429 Murder                                3\n16  2017 BURGLARY                  13001 Theft                                 1\n17  2017 HOMICIDE                    676 Murder                                1\n18  2018 HOMICIDE                    600 Murder                                1\n19  2019 HOMICIDE                    509 Murder                                1\n20  2020 HOMICIDE                    796 Murder                                1\n\n\nTo prepare the exoneration dataset for comparison with Chicago crime data, we began by filtering the data to include only individuals convicted in Cook County from the year 2001 onward, aligning with the temporal scope of the crime dataset. We grouped the filtered data by conviction year and the variable Worst Crime Display, which indicates the most serious offense in each case, and calculated the number of exonerations per year for each crime type. To enable a valid comparison, we created a mapping table that aligns exoneration crime categories (e.g., “Murder”, “Sexual Assault”) with the Primary Type categories used in the Chicago crime dataset (e.g., “HOMICIDE”, “CRIM SEXUAL ASSAULT”). After applying this mapping, we joined the exoneration summary with the cleaned and aggregated Chicago crime data using both year and crime type as keys. The resulting combined dataset allows for side-by-side comparisons of how many people were convicted of certain serious crimes each year and how many were later exonerated, enabling a structured analysis of trends and disparities in the justice system over time.\n\nWhether spikes in reported crimes correlate with spikes in wrongful convictions a few years later.\n\nFrom the chart, there is no clear or consistent lagged correlation between spikes in crime reports and spikes in exonerations. For instance, HOMICIDE counts remain high and relatively stable across the early 2000s (e.g., 667 in 2001, 658 in 2002, 604 in 2003), while exoneration counts for those same conviction years fluctuate without a clear pattern (7 in 2001, 4 in 2002, 2 in 2003). Similarly, BURGLARY had high crime counts (25,623 in 2002), but very few corresponding exonerations (only 1). This suggests that while wrongful convictions are happening, they are not necessarily directly following spikes in reported crime at a systemic or predictable lag.\n\nThe types of crimes most commonly associated with exonerations in Cook County, and how those compare to the overall crime landscape in Chicago.\n\nThe data shows that HOMICIDE (mapped to “Murder”) is by far the most commonly associated crime with exonerations in Cook County. Nearly all years shown in the chart have homicide-related exonerations, with counts ranging from 2 to 7 per year. On the other hand, BURGLARY, while frequently reported (e.g., ~26,000 incidents per year), results in almost no exonerations. Similarly, ROBBERY and SEX OFFENSE categories have few to no exoneration entries despite high occurrence rates. This contrast suggests a disproportionate concentration of wrongful convictions in the most severe crimes, like homicides and sexual assaults. These are often cases with longer sentences and higher investigative scrutiny, where flawed evidence (e.g., eyewitness misidentification, coerced confessions, or prosecutorial misconduct) may have played a significant role and where advocacy efforts focus more intensively."
  },
  {
    "objectID": "posts/2025-03-31-Blog-Post-4/BlogPost4.html",
    "href": "posts/2025-03-31-Blog-Post-4/BlogPost4.html",
    "title": "Blog Post 4",
    "section": "",
    "text": "In our previous blog posts, we established foundational insights from the National Registry of Exonerations dataset. We uncovered broad trends—for example, that Black exonerees are overrepresented, homicide dominates the dataset, and that DNA evidence and false confessions appear disproportionately in specific crimes.This week, we began moving from exploratory analysis to explanatory analysis. Rather than simply observing that disparities exist, we aimed to dig deeper: · What factors predict whether false evidence is involved in a case? · Do these factors vary by race or crime type? · Can we start to model these outcomes with statistical tools?\nWe focused this week’s exploratory analysis on the role of false evidence (including official misconduct, perjury/false accusation, and false confession). We wanted to understand how often this occurs, and whether it disproportionately affects certain racial groups or certain crime types. We created a new binary column, False_Evidence, defined as cases with any of the following tags: · OM (Official Misconduct) · P/FA (Perjury or False Accusation) · FC (False Confession) Using this, we calculated the proportion of exonerees within each racial group whose cases involved false evidence.\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(ggplot2)\n# Load the cleaned exoneration dataset\nexoneration_data &lt;- readRDS(here::here(\"dataset\", \"exoneration_data_clean.rds\"))\n# Create False_Evidence column\nexoneration_data &lt;- exoneration_data %&gt;%\n  mutate(False_Evidence = if_else(!is.na(`OM`) | !is.na(`P/FA`) | !is.na(`FC`), 1, 0))\n\n# Plot: False Evidence by Race\nexoneration_data %&gt;%\n  group_by(Race) %&gt;%\n  summarize(FE_rate = mean(False_Evidence, na.rm = TRUE)) %&gt;%\n  ggplot(aes(x = reorder(Race, FE_rate), y = FE_rate)) +\n  geom_col(fill = \"darkgreen\") +\n  coord_flip() +\n  labs(title = \"Proportion of False Evidence Cases by Race\",\n       x = \"Race\", y = \"Proportion\") +\n  scale_y_continuous(labels = scales::percent)\n\n\n\n\n\n\n\n\nFigure 1 Insight: False Evidence Disparities by Race This chart shows the proportion of exonerees in each racial group whose cases involved false evidence—defined as the presence of official misconduct (OM), false confessions (FC), or perjury/false accusation (P/FA). · The highest proportion is found in the “Don’t Know” group, likely due to data entry uncertainty or legacy cases with incomplete documentation. · Among named racial groups, Black, Hispanic, and White exonerees all show relatively high proportions of false evidence cases—over 65% in each group. · Black exonerees have the highest known rate of false evidence involvement, slightly above Hispanic and White counterparts. This finding supports the hypothesis that racial disparities exist not only in wrongful convictions themselves but also in the mechanisms that cause them. · Asian, Native American, and “Other” groups show lower proportions, though these groups have smaller sample sizes, which may contribute to more variation or underreporting. Overall, this plot reinforces the importance of examining race as a structural factor influencing how and why wrongful convictions occur.\nWe also explored whether DNA evidence is more common in some types of crimes than others.\n\n# Plot: DNA Evidence by Worst Crime Type\nexoneration_data %&gt;%\n  mutate(DNA_binary = if_else(!is.na(DNA), 1, 0)) %&gt;%\n  group_by(Worst_Crime_Display) %&gt;%\n  summarize(Count = n(),\n            DNA_Rate = mean(DNA_binary, na.rm = TRUE)) %&gt;%\n  arrange(desc(Count)) %&gt;%\n  top_n(12, Count) %&gt;%\n  ggplot(aes(x = reorder(Worst_Crime_Display, DNA_Rate), y = DNA_Rate)) +\n  geom_col(fill = \"steelblue\") +\n  coord_flip() +\n  labs(title = \"DNA Evidence Rate by Crime Type\",\n       x = \"Crime Type\", y = \"Proportion with DNA Evidence\") +\n  scale_y_continuous(labels = scales::percent)\n\n\n\n\n\n\n\n\nFigure 2 Insight: DNA Evidence Varies Strongly by Crime Type\nThis plot illustrates how the use of DNA evidence in exoneration cases differs significantly across crime types. · Sexual assault cases overwhelmingly stand out, with over 65% of cases involving DNA evidence. This is likely due to the nature of such crimes, where biological evidence plays a central role in both prosecution and exoneration. · Murder and manslaughter follow as the next highest categories, each with around 30–40% DNA involvement, reflecting their severity and the likelihood of forensic investigation. · Crimes such as fraud, drug possession, and weapon-related offenses show very low rates of DNA evidence, typically below 5%, indicating that wrongful convictions in these categories are more likely driven by non-forensic factors like false accusations, misidentification, or official misconduct. · There’s a clear distinction between violent crimes (with high DNA rates) and non-violent or administrative crimes (with low or no DNA involvement).\nThese differences highlight the importance of contextualizing wrongful conviction mechanisms by crime type, and suggest that forensic evidence plays a far greater role in rectifying wrongful convictions for violent offenses.\nAs we begin modeling, we outlined a list of potential response variables and predictors, along with possible transformations to improve interpretability and predictive power：\nPotential Response Variables: Convicted / Exonerated – binary response (Yes/No) Posting Date / Date of Crime Year – can be used to compute “time to exoneration” or analyze trends over time Worst Crime Display – could be used as a categorical response for classification F/MFE (Female/Male False Evidence) – whether false evidence was involved (binary response)\nPotential Predictor Variables: Age Race Sex State / County Type of Crime (Worst Crime Display) Date of Crime Year DNA / MWID / OM Tags\nPossible Transformations: Binarize tags: turn tags like P/FA, DNA, MWID, etc. into separate binary columns (0 or 1) Compute sentence length: transform sentence text (“10 years”, “Life without parole”, etc.) into numeric values (e.g., Life = 99, or just flag it separately) Date differences: derive “time between conviction and posting”, or “time to exoneration” Group ages: create age groups (under 18, 18–25, 26–35, etc.) Simplify crimes: group crimes into broader categories (e.g., violent vs. non-violent)\nModeling Options: 1. Logistic Regression (binary outcomes) Predicting whether someone is exonerated (Yes/No) Predicting presence of DNA evidence or false evidence Predicting whether someone receives life sentence\n\nLinear Regression (continuous outcomes) Predicting sentence length (in years)\n\n\n# Prepare data\nmodel_data &lt;- exoneration_data %&gt;%\n  mutate(False_Evidence = if_else(!is.na(`OM`) | !is.na(`P/FA`) | !is.na(`FC`), 1, 0),\n         DNA_binary = if_else(!is.na(DNA), 1, 0),\n         MWID_binary = if_else(!is.na(MWID), 1, 0)) %&gt;%\n  filter(!is.na(False_Evidence), !is.na(Race), !is.na(Sex), !is.na(Age), !is.na(Worst_Crime_Display))\n\n# Fit model\nlogit_model &lt;- glm(False_Evidence ~ Race + Sex + Age + Worst_Crime_Display + DNA_binary + MWID_binary,\n                   data = model_data, family = binomial)\n\n# Output summary\nsummary(logit_model)\n\n\nCall:\nglm(formula = False_Evidence ~ Race + Sex + Age + Worst_Crime_Display + \n    DNA_binary + MWID_binary, family = binomial, data = model_data)\n\nCoefficients:\n                                                   Estimate Std. Error z value\n(Intercept)                                        16.02742  827.63812   0.019\nRaceBlack                                           1.03866    0.46944   2.213\nRaceDon't Know                                     16.96728  933.93609   0.018\nRaceHispanic                                        0.82617    0.48066   1.719\nRaceNative American                                 0.11322    0.65442   0.173\nRaceOther                                          -0.19038    0.71371  -0.267\nRaceWhite                                           0.47518    0.46977   1.012\nSexMale                                             0.73772    0.14830   4.975\nAge                                                -0.01152    0.00472  -2.440\nWorst_Crime_DisplayArson                          -16.36825  827.63808  -0.020\nWorst_Crime_DisplayAssault                        -14.98970  827.63801  -0.018\nWorst_Crime_DisplayAttempt, Nonviolent            -17.01855  827.63932  -0.021\nWorst_Crime_DisplayAttempt, Violent               -14.33418  827.63839  -0.017\nWorst_Crime_DisplayAttempted Murder               -15.19084  827.63801  -0.018\nWorst_Crime_DisplayBribery                        -17.02215  827.63835  -0.021\nWorst_Crime_DisplayBurglary/Unlawful Entry        -15.91454  827.63809  -0.019\nWorst_Crime_DisplayChild Abuse                    -16.54550  827.63813  -0.020\nWorst_Crime_DisplayChild Sex Abuse                -14.22965  827.63800  -0.017\nWorst_Crime_DisplayConspiracy                     -17.01638  827.63834  -0.021\nWorst_Crime_DisplayDependent Adult Abuse          -33.28804 2538.26702  -0.013\nWorst_Crime_DisplayDestruction of Property        -16.71760  827.63902  -0.020\nWorst_Crime_DisplayDrug Possession or Sale        -16.48286  827.63798  -0.020\nWorst_Crime_DisplayFailure to Pay Child Support   -33.54563 1864.89364  -0.018\nWorst_Crime_DisplayFiling a False Report            0.35118 2538.26702   0.000\nWorst_Crime_DisplayForgery                        -15.51443  827.63896  -0.019\nWorst_Crime_DisplayFraud                          -15.78224  827.63803  -0.019\nWorst_Crime_DisplayHarassment                       0.24061 2538.26707   0.000\nWorst_Crime_DisplayImmigration                     -0.54408 1456.59414   0.000\nWorst_Crime_DisplayKidnapping                     -15.69726  827.63811  -0.019\nWorst_Crime_DisplayManslaughter                   -15.78125  827.63802  -0.019\nWorst_Crime_DisplayMenacing                        -0.28608 1883.39475   0.000\nWorst_Crime_DisplayMilitary Justice Offense        -0.83457 2538.26702   0.000\nWorst_Crime_DisplayMurder                         -14.44534  827.63798  -0.017\nWorst_Crime_DisplayObstruction of Justice         -15.36944  827.63892  -0.019\nWorst_Crime_DisplayOfficial Misconduct            -15.46930  827.63880  -0.019\nWorst_Crime_DisplayOther                          -32.88000 2538.26707  -0.013\nWorst_Crime_DisplayOther Nonviolent Felony        -17.42766  827.63809  -0.021\nWorst_Crime_DisplayOther Nonviolent Misdemeanor   -16.10496  827.63825  -0.019\nWorst_Crime_DisplayOther Violent Felony           -15.81255  827.63815  -0.019\nWorst_Crime_DisplayOther Violent Misdemeanor      -15.56821  827.63871  -0.019\nWorst_Crime_DisplayPerjury                         -0.28690 1352.64415   0.000\nWorst_Crime_DisplayPossession of Stolen Property  -17.36897  827.63903  -0.021\nWorst_Crime_DisplayRobbery                        -15.54097  827.63800  -0.019\nWorst_Crime_DisplaySex Offender Registration      -33.53812  906.90208  -0.037\nWorst_Crime_DisplaySexual Assault                 -15.23539  827.63798  -0.018\nWorst_Crime_DisplaySolicitation                   -16.01731  827.63888  -0.019\nWorst_Crime_DisplayStalking                       -16.41285  827.63891  -0.020\nWorst_Crime_DisplaySupporting Terrorism             0.18933 1453.59670   0.000\nWorst_Crime_DisplayTax Evasion/Fraud              -14.84393  827.63869  -0.018\nWorst_Crime_DisplayTheft                          -16.57169  827.63809  -0.020\nWorst_Crime_DisplayThreats                        -17.13255  827.63834  -0.021\nWorst_Crime_DisplayTraffic Offense                -15.71428  827.63828  -0.019\nWorst_Crime_DisplayWeapon Possession or Sale      -16.15556  827.63802  -0.020\nDNA_binary                                         -0.45439    0.13454  -3.377\nMWID_binary                                        -1.71540    0.12540 -13.680\n                                                 Pr(&gt;|z|)    \n(Intercept)                                      0.984550    \nRaceBlack                                        0.026930 *  \nRaceDon't Know                                   0.985505    \nRaceHispanic                                     0.085647 .  \nRaceNative American                              0.862648    \nRaceOther                                        0.789666    \nRaceWhite                                        0.311769    \nSexMale                                          6.54e-07 ***\nAge                                              0.014679 *  \nWorst_Crime_DisplayArson                         0.984221    \nWorst_Crime_DisplayAssault                       0.985550    \nWorst_Crime_DisplayAttempt, Nonviolent           0.983594    \nWorst_Crime_DisplayAttempt, Violent              0.986182    \nWorst_Crime_DisplayAttempted Murder              0.985356    \nWorst_Crime_DisplayBribery                       0.983591    \nWorst_Crime_DisplayBurglary/Unlawful Entry       0.984659    \nWorst_Crime_DisplayChild Abuse                   0.984050    \nWorst_Crime_DisplayChild Sex Abuse               0.986283    \nWorst_Crime_DisplayConspiracy                    0.983597    \nWorst_Crime_DisplayDependent Adult Abuse         0.989536    \nWorst_Crime_DisplayDestruction of Property       0.983885    \nWorst_Crime_DisplayDrug Possession or Sale       0.984111    \nWorst_Crime_DisplayFailure to Pay Child Support  0.985648    \nWorst_Crime_DisplayFiling a False Report         0.999890    \nWorst_Crime_DisplayForgery                       0.985044    \nWorst_Crime_DisplayFraud                         0.984786    \nWorst_Crime_DisplayHarassment                    0.999924    \nWorst_Crime_DisplayImmigration                   0.999702    \nWorst_Crime_DisplayKidnapping                    0.984868    \nWorst_Crime_DisplayManslaughter                  0.984787    \nWorst_Crime_DisplayMenacing                      0.999879    \nWorst_Crime_DisplayMilitary Justice Offense      0.999738    \nWorst_Crime_DisplayMurder                        0.986075    \nWorst_Crime_DisplayObstruction of Justice        0.985184    \nWorst_Crime_DisplayOfficial Misconduct           0.985088    \nWorst_Crime_DisplayOther                         0.989665    \nWorst_Crime_DisplayOther Nonviolent Felony       0.983200    \nWorst_Crime_DisplayOther Nonviolent Misdemeanor  0.984475    \nWorst_Crime_DisplayOther Violent Felony          0.984757    \nWorst_Crime_DisplayOther Violent Misdemeanor     0.984992    \nWorst_Crime_DisplayPerjury                       0.999831    \nWorst_Crime_DisplayPossession of Stolen Property 0.983257    \nWorst_Crime_DisplayRobbery                       0.985019    \nWorst_Crime_DisplaySex Offender Registration     0.970500    \nWorst_Crime_DisplaySexual Assault                0.985313    \nWorst_Crime_DisplaySolicitation                  0.984559    \nWorst_Crime_DisplayStalking                      0.984178    \nWorst_Crime_DisplaySupporting Terrorism          0.999896    \nWorst_Crime_DisplayTax Evasion/Fraud             0.985690    \nWorst_Crime_DisplayTheft                         0.984025    \nWorst_Crime_DisplayThreats                       0.983485    \nWorst_Crime_DisplayTraffic Offense               0.984852    \nWorst_Crime_DisplayWeapon Possession or Sale     0.984426    \nDNA_binary                                       0.000731 ***\nMWID_binary                                       &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 4005.5  on 3630  degrees of freedom\nResidual deviance: 3271.1  on 3576  degrees of freedom\nAIC: 3381.1\n\nNumber of Fisher Scoring iterations: 15\n\n\nKey Findings:\nRace: Being Black was significantly associated with higher odds of false evidence involvement (p = 0.027), controlling for other variables. Hispanic exonerees also showed a marginally significant effect (p ≈ 0.086). Other racial groups did not differ significantly from the reference category (likely Asian or Native American, depending on how factors were encoded).\nSex and Age: Male exonerees were significantly more likely to have false evidence involved in their case (p &lt; 0.001).Older age was associated with slightly lower odds of false evidence (p = 0.015), though the effect size is small.\nCrime Type: None of the crime types reached statistical significance. This may be due to multicollinearity or sparse data in many crime categories. Some estimates are unusually large or small with very high standard errors, suggesting overparameterization or poor category separation.\nDNA and MWID Evidence: Interestingly, the presence of DNA evidence was negatively associated with false evidence (estimate = -0.454, p = 0.0007). This could suggest that cases involving false evidence may lack concrete forensic support.Mistaken witness identification (MWID) had the strongest negative association with false evidence (estimate = -1.72, p &lt; 0.0001), which is intuitive—if a case hinges on mistaken ID, it may be less likely to also involve fabricated evidence.\nModel Fit: The residual deviance decreased from 4005.5 to 3271.1, indicating improved fit. AIC = 3381.1, which can be used for model comparison later."
  },
  {
    "objectID": "posts/2025-03-17-Blog-Post-2/BlogPost2.html",
    "href": "posts/2025-03-17-Blog-Post-2/BlogPost2.html",
    "title": "Blog Post 2",
    "section": "",
    "text": "Data background\nThe data comes from the National Registry of Exonerations website, which is a collaborative project led by the Newkirk Center for Science & Society. The data is collected by researchers at the University of California Irvine, the University Michigan Law School, and the Michigan State University College of Law from court documents, news reports, and advocacy organizations, and other public sources to track whether a person’s conviction is overturned. This dataset is the original source as far as exoneration data goes.\nThere’s no issue with how the data was collected. The sample population is the group of people who have been officially exonerated, but it’s not a record of all wrongful convictions, just specifically cases in which someone was able to obtain an exoneration. Because the dataset only includes cases that achieved an exoneration, there’s probability that a lot of wrongful convictions never come to attention. Also, because of the publicly available information, some cases may have limited documentation, especially some older ones with fewer resources and publicity.\nThis data is used to research and analyze the causes and impacts of wrongful convictions, and its findings reflect systemic issues within the criminal justice system. Additionally, it provides a reference for policy-making to help prevent and reduce wrongful convictions while offering support for governance.Some other studies have also utilized the same data. For example, the American Statistical Association (ASA)’s Law and Justice Statistics Section has analyzed the frequency and distribution of wrongful convictions. The research article “Just Data: Advancing the Innocence Movement” also discusses the practical applications of NRE data in real life.The data provided by NRE has been used for policy decisions and has played a significant role in the development of criminal justice policies. Based on NRE data, researchers and policymakers have raised several important questions, such as: How does racial bias influence wrongful convictions? What reforms should be implemented to reduce such injustices? What are the common factors leading to wrongful convictions? The NRE data provides direction for improving the judicial system and helps prevent future miscarriages of justice.\nTo obtain the data, go to https://www.law.umich.edu/special/exoneration/Pages/Spread-Sheet-Request-Form.aspx and fill out the form using your name and email. Shortly after, the website provides you with an Excel sheet which can be converted to a CSV file. We have included the CSV file in the dataset folder (our dataset is less than 50 MB). We also explored some ways to clean the data in the clean_data.R file in the scripts folder. For the most part, the data is already pretty easy to work with. Our cleaning script performs several key operations on the exoneration dataset. First, it renames column names by replacing spaces with underscores for easier access. It then converts the Age column to an integer, which may result in missing values (NA) if non-numeric entries exist. The Date of Crime Year column is cleaned by removing commas and converting it to an integer. Next, the script replaces all missing values with NA across the dataset, which may affect categorical variables like Tags or Worst Crime Display that might need “Unknown” instead."
  },
  {
    "objectID": "posts/2024-10-04-general-tips/general-tips.html",
    "href": "posts/2024-10-04-general-tips/general-tips.html",
    "title": "General Tips",
    "section": "",
    "text": "Use the tidyverse!\nYou don’t have to tell me what kind of chart something is. For example, the below is not a useful start to a sentence.\n\n\nThe graph presents a horizontal bar chart …\n\n\nEach page should be largely standalone.\nSometimes small tables or even inline numbers are better than a figure.\nRedundant colors (e.g. bar charts where each bar is a different color that doesn’t signify anything) often don’t help.\nProvide some details on how much data was removed in your cleaning process.\nUse the tidyverse!\nImagine I’m an impatient boss. Show me only what is important and relevant.\nCleaning must be entirely in R\nDon’t say things like, well if only everyone did like so and so than everything would be better. There are many things hiding behind the data that would go to explain things. This is an example of a bad conclusion.\n\n\nThe world could benefit form modeling its education systems after Europe’s.\n\nIt is fine to talk about how the European system is better according to certain metrics, but don’t assume that can easily translate to other regions.\n\nDon’t talk about your “journey”. The blog posts tell the story of your journey. The main pages should focus on the data and your findings.\n\n\nUse the tidyverse!\nNo but seriously, when asking ChatGPT to do your project for you, make sure to tell it to use the tidyverse, not base R."
  },
  {
    "objectID": "posts/2023-10-15-getting-started/getting-started.html",
    "href": "posts/2023-10-15-getting-started/getting-started.html",
    "title": "Getting started",
    "section": "",
    "text": "Below, the items marked with [[OP]] should only be done by one person on the team.\n\nTo get started\n\n[[OP]] One person from the team should click the Github Classroom link on Teams.\n[[OP]] That person types in the group name for their group.\nThe rest of the team now clicks the Github Classroom link and selects their team from the dropdown list.\nFinally, each of you can clone the repository to your laptop like a normal assignment.\n\n\n\nSetting up the site\n\n[[OP]] Open the terminal and run quarto publish gh-pages.\n[[OP]] Select Yes to the prompt:  ? Publish site to https://sussmanbu.github.io/ma4615-fa23-final-project-TEAMNAME/ using gh-pages? (Y/n)\n[[OP]] Wait for the process to finish.\nOnce it is done, you can go to the URL it asked you about to see your site.\n\nNote: This is the process you will use every time you want to update your published site. Make sure to always follow the steps below for rendering, previewing, and committing your changes before doing these publish steps. Anyone can publish in the future.\n\n\nCustomize your site\n\n[[OP]] Open the _quarto.yml file and update the title to include your team name.\n[[OP]] Go to the about.qmd and remove the TF’s and professor’s names.\nadd your own along with a short introduction and a link to your Github user page.\n[[OP]] Render the site.\n[[OP]] Check and make sure you didn’t get any errors.\n[[OP]] Commit your changes and push.\n[[OP]] Repeat the steps under Setting up your site.\n\nOnce one person is done with this, each teammate in the group can, in turn, repeat steps 3-7. Before doing so, make sure to pull the changes from teammates before starting to make new changes. (We’ll talk soon about ways to organize your work and resolve conflicts.)\n\n\nStart your first post\n\nTo start your first post first, run remotes::install_github(\"sussmanbu/quartopost\") in your Console.\n[[OP]] Run quartopost::quartopost() (or click Addins-&gt;Create Quarto Post, or use C-Shift-P, type “Create Quarto” and press enter to run the command).\n\nNow you can start working on your post. You’ll want to render your post to see what it will look like on the site.\n\nEvery time you want to make a new post, you can repeat step 2 above.\nWhen you want to publish your progress, follow steps 4-7 from Customize your site.\n\nFinally, make sure to read through everything on this site which has the directions and rubric for the final project."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "MA [46]15 Final Project",
    "section": "",
    "text": "Final Project due May 5, 2024 at 11:59pm.\nThis comes from the index.qmd file.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBlogPost 7\n\n\n\n\n\nOur seventh blog post containing the combination of two datasets \n\n\n\n\n\nApr 22, 2025\n\n\nTeam 10 (Mango)\n\n\n\n\n\n\n\n\n\n\n\n\nBlogPost 6\n\n\n\n\n\nOur sixth blog post containing the combination of two datasets \n\n\n\n\n\nApr 14, 2025\n\n\nTeam 10 (Mango)\n\n\n\n\n\n\n\n\n\n\n\n\nBlog Post 5\n\n\n\n\n\nOur fifth blog post containing details about our second dataset. \n\n\n\n\n\nApr 7, 2025\n\n\nTeam 10 (Mango)\n\n\n\n\n\n\n\n\n\n\n\n\nBlog Post 4\n\n\n\n\n\nOur fourth blog post containing a more in depth analysis of the exoneration data set. \n\n\n\n\n\nMar 31, 2025\n\n\nTeam 10 (Mango)\n\n\n\n\n\n\n\n\n\n\n\n\nBlog Post 3\n\n\n\n\n\nOur third blog post containing progress on data cleaning. \n\n\n\n\n\nMar 24, 2025\n\n\nTeam 10 (Mango)\n\n\n\n\n\n\n\n\n\n\n\n\nBlog Post 2\n\n\n\n\n\nOur second blog post containing our progress so far. \n\n\n\n\n\nMar 17, 2025\n\n\nTeam 10 (Mango)\n\n\n\n\n\n\n\n\n\n\n\n\nBlog Post 1\n\n\n\n\n\nOur first blog post containing 3 proposed data sets. \n\n\n\n\n\nMar 3, 2025\n\n\nTeam 10 (Mango)\n\n\n\n\n\n\n\n\n\n\n\n\nGeneral Tips\n\n\n\n\n\nSome small but important tips to follow. \n\n\n\n\n\nOct 4, 2024\n\n\nDaniel Sussman\n\n\n\n\n\n\n\n\n\n\n\n\nExamples\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 26, 2024\n\n\nDaniel Sussman\n\n\n\n\n\n\n\n\n\n\n\n\nGetting started\n\n\n\n\n\n\n\n\nDirections to set up your website and create your first post. \n\n\n\n\n\nFeb 23, 2024\n\n\nDaniel Sussman\n\n\n\n\n\n\n\n\n\n\n\n\nFirst Team Meeting\n\n\n\n\n\n\n\n\nThis post details the steps you’ll take for your first team meeting. \n\n\n\n\n\nFeb 21, 2024\n\n\nDaniel Sussman\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "big_picture.html",
    "href": "big_picture.html",
    "title": "Big Picture",
    "section": "",
    "text": "This comes from the file big_picture.qmd.\nThink of this page as your 538/Upshot style article. This means that you should try to tell a story through the data and your analysis. Read articles from those sites and similar sites to get a feeling for what they are like. Try to write in the style of a news or popular article. Importantly, this page should be geared towards the general public. You shouldn’t assume the reader understands how to interpret a linear regression or a complicated plot. Focus on interpretation and visualizations."
  },
  {
    "objectID": "big_picture.html#rubric-on-this-page",
    "href": "big_picture.html#rubric-on-this-page",
    "title": "Big Picture",
    "section": "Rubric: On this page",
    "text": "Rubric: On this page\n\nTitle\n\nYour big picture page should have a creative/click-bait-y title/headline that provides a hint about your thesis.\n\nClarity of Explanation\n\nYou should have a clear thesis/goal for this page. What are you trying to show? Make sure that you explain your analysis in detail but don’t go into top much mathematics or statistics. The audience for this page is the general public (to the extent possible). Your thesis should be a statement, not a question.\nEach figure should be very polished and also not too complicated. There should be a clear interpretation of the figure so the figure has a clear purpose. Even something like a histogram can be difficult to interpret for non-experts.\n\nCreativity\n\nDo your best to make things interesting. Think of a how a news article or a magazine story might draw you in. Think of how each part of your analysis supports the previous part or provides a different perspective.\n\nInteractive component\n\nQuality and ease of use of the interactive components. Is it clear what can be explored using your interactive components? Does it enhance and reinforce your conclusions?\n\nThis page should be self-contained.\n\nNote: This page should have no code visible, i.e. use #| echo: FALSE."
  },
  {
    "objectID": "big_picture.html#rubric-other-components",
    "href": "big_picture.html#rubric-other-components",
    "title": "Big Picture",
    "section": "Rubric: Other components",
    "text": "Rubric: Other components\n\nVideo Recording\nMake a video recording (probably using Zoom) demonstrating your interactive components. You should provide a quick explanation of your data and demonstrate some of the conclusions from your EDA. This video should be no longer than 4 minutes. Include a link to your video (and password if needed) in your README.md file on your Github repository. You are not required to provide a link on the website. This can be presented by any subset of the team members.\n\n\nRest of the Site\nFinally, here are important things to keep in mind for the rest of the site.\nThe main title of your page is informative. Each post has an author/description/informative title. All lab required posts are present. Each page (including the home page) has a nice featured image associated with it. Your about page is up to date and clean. You have removed the generic posts from the initial site template."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This comes from the file about.qmd.\nProject Github: https://github.com/sussmanbu/ma4615-sp25-final-project-team-10\n:)))\nThis is a website for the final project for MA[46]15 Data Science with R by Team Mango. The members of this team are below."
  },
  {
    "objectID": "about.html#zuizz-saeed-httpsgithub.comzuizzms",
    "href": "about.html#zuizz-saeed-httpsgithub.comzuizzms",
    "title": "About",
    "section": "Zuizz Saeed (https://github.com/zuizzms)",
    "text": "Zuizz Saeed (https://github.com/zuizzms)\nZuizz is a Computer Science student at Boston University and is graduating in May 2025."
  },
  {
    "objectID": "about.html#jessica-park-httpsgithub.comjessp24",
    "href": "about.html#jessica-park-httpsgithub.comjessp24",
    "title": "About",
    "section": "Jessica Park (https://github.com/jessp24)",
    "text": "Jessica Park (https://github.com/jessp24)\nJessica is a Data Science student at Boston University and is graduating in May 2026."
  },
  {
    "objectID": "about.html#mohan-wang-httpsgithub.commohanw792",
    "href": "about.html#mohan-wang-httpsgithub.commohanw792",
    "title": "About",
    "section": "Mohan Wang (https://github.com/MohanW792)",
    "text": "Mohan Wang (https://github.com/MohanW792)\nMohan is a Data Science student at Boston University and is graduating in May 2026."
  },
  {
    "objectID": "about.html#sitong-lu-httpsgithub.comtammylu",
    "href": "about.html#sitong-lu-httpsgithub.comtammylu",
    "title": "About",
    "section": "Sitong Lu (https://github.com/Tammylu)",
    "text": "Sitong Lu (https://github.com/Tammylu)\nTammy is a Data Science student at Boston University and is graduating in May 2026."
  },
  {
    "objectID": "about.html#radhika-jhunjhunwala-httpsgithub.comradhika211103",
    "href": "about.html#radhika-jhunjhunwala-httpsgithub.comradhika211103",
    "title": "About",
    "section": "Radhika Jhunjhunwala (https://github.com/radhika211103)",
    "text": "Radhika Jhunjhunwala (https://github.com/radhika211103)\nRadhika is a Mathematical Economics student at Boston University and is graduating in May 2026.\n\n\nAbout this Template.\nThis is based off of the standard Quarto website template from RStudio (2023.09.0 Build 463)."
  },
  {
    "objectID": "analysis.html",
    "href": "analysis.html",
    "title": "Analysis",
    "section": "",
    "text": "This comes from the file analysis.qmd.\nWe describe here our detailed data analysis. This page will provide an overview of what questions you addressed, illustrations of relevant aspects of the data with tables and figures, and a statistical model that attempts to answer part of the question. You’ll also reflect on next steps and further analysis.\nThe audience for this page is someone like your class mates, so you can expect that they have some level of statistical and quantitative sophistication and understand ideas like linear and logistic regression, coefficients, confidence intervals, overfitting, etc.\nWhile the exact number of figures and tables will vary and depend on your analysis, you should target around 5 to 6. An overly long analysis could lead to losing points. If you want you can link back to your blog posts or create separate pages with more details.\nThe style of this paper should aim to be that of an academic paper. I don’t expect this to be of publication quality but you should keep that aim in mind. Avoid using “we” too frequently, for example “We also found that …”. Describe your methodology and your findings but don’t describe your whole process."
  },
  {
    "objectID": "analysis.html#note-on-attribution",
    "href": "analysis.html#note-on-attribution",
    "title": "Analysis",
    "section": "Note on Attribution",
    "text": "Note on Attribution\nIn general, you should try to provide links to relevant resources, especially those that helped you. You don’t have to link to every StackOverflow post you used but if there are explainers on aspects of the data or specific models that you found helpful, try to link to those. Also, try to link to other sources that might support (or refute) your analysis. These can just be regular hyperlinks. You don’t need a formal citation.\nIf you are directly quoting from a source, please make that clear. You can show long quotes using &gt; like this\n&gt; To be or not to be.\n\nTo be or not to be."
  },
  {
    "objectID": "analysis.html#rubric-on-this-page",
    "href": "analysis.html#rubric-on-this-page",
    "title": "Analysis",
    "section": "Rubric: On this page",
    "text": "Rubric: On this page\nYou will\n\nIntroduce what motivates your Data Analysis (DA)\n\nWhich variables and relationships are you most interested in?\nWhat questions are you interested in answering?\nProvide context for the rest of the page. This will include figures/tables that illustrate aspects of the data of your question.\n\nModeling and Inference\n\nThe page will include some kind of formal statistical model. This could be a linear regression, logistic regression, or another modeling framework.\nExplain the ideas and techniques you used to choose the predictors for your model. (Think about including interaction terms and other transformations of your variables.)\nDescribe the results of your modelling and make sure to give a sense of the uncertainty in your estimates and conclusions.\n\nExplain the flaws and limitations of your analysis\n\nAre there some assumptions that you needed to make that might not hold? Is there other data that would help to answer your questions?\n\nClarity Figures\n\nAre your figures/tables/results easy to read, informative, without problems like overplotting, hard-to-read labels, etc?\nEach figure should provide a key insight. Too many figures or other data summaries can detract from this. (While not a hard limit, around 5 total figures is probably a good target.)\nDefault lm output and plots are typically not acceptable.\n\nClarity of Explanations\n\nHow well do you explain each figure/result?\nDo you provide interpretations that suggest further analysis or explanations for observed phenomenon?\n\nOrganization and cleanliness.\n\nMake sure to remove excessive warnings, hide all code, organize with sections or multiple pages, use bullets, etc.\nThis page should be self-contained, i.e. provide a description of the relevant data."
  },
  {
    "objectID": "data.html",
    "href": "data.html",
    "title": "Data",
    "section": "",
    "text": "This comes from the file data.qmd.\nYour first steps in this project will be to find data to work on.\nI recommend trying to find data that interests you and that you are knowledgeable about. A bad example would be if you have no interest in video games but your data set is about video games. I also recommend finding data that is related to current events, social justice, and other areas that have an impact.\nInitially, you will study one dataset but later you will need to combine that data with another dataset. For this reason, I recommend finding data that has some date and/or location components. These types of data are conducive to interesting visualizations and analysis and you can also combine this data with other data that also has a date or location variable. Data from the census, weather data, economic data, are all relatively easy to combine with other data with time/location components."
  },
  {
    "objectID": "data.html#what-makes-a-good-data-set",
    "href": "data.html#what-makes-a-good-data-set",
    "title": "Data",
    "section": "What makes a good data set?",
    "text": "What makes a good data set?\n\nData you are interested in and care about.\nData where there are a lot of potential questions that you can explore.\nA data set that isn’t completely cleaned already.\nMultiple sources for data that you can combine.\nSome type of time and/or location component."
  },
  {
    "objectID": "data.html#where-to-keep-data",
    "href": "data.html#where-to-keep-data",
    "title": "Data",
    "section": "Where to keep data?",
    "text": "Where to keep data?\nBelow 50mb: In dataset folder\nAbove 50mb: In dataset-ignore folder which you will have to create manually. This folder will be ignored by git so you’ll have to manually sync these files across your team.\n\nSharing your data\nFor small datasets (&lt;50mb), you can use the dataset folder that is tracked by github. Stage and commit the files just like you would any other file.\nFor larger datasets, you’ll need to create a new folder in the project root directory named dataset-ignore. This will be ignored by git (based off the .gitignore file in the project root directory) which will help you avoid issues with Github’s size limits. Your team will have to manually make sure the data files in dataset-ignore are synced across team members.\nYour clean_data.R file in the scripts folder is the file where you will import the raw data that you download, clean it, and write .rds file(s) (using write_rds) that you’ll load in your analysis page. If desirable, you can have multiple scripts that produce different derived data sets, just make sure to link to them on this page.\nYou should never use absolute paths (eg. /Users/danielsussman/path/to/project/ or C:\\MA415\\\\Final_Project\\). Instead, use the here function from the here package to avoid path problems.\n\n\nClean data script\nThe idea behind this file is that someone coming to your website could largely replicate your analyses after running this script on the original data sets to clean them. This file might create a derivative data set that you then use for your subsequent analysis. Note that you don’t need to run this script from every post/page. Instead, you can load in the results of this script, which will usually be .rds files. In your data page you’ll describe how these results were created. If you have a very large data set, you might save smaller data sets that you can use for exploration purposes. To link to this file, you can use [cleaning script](/scripts/clean_data.R) wich appears as cleaning script."
  },
  {
    "objectID": "data.html#rubric-on-this-page",
    "href": "data.html#rubric-on-this-page",
    "title": "Data",
    "section": "Rubric: On this page",
    "text": "Rubric: On this page\nYou will\n\nDescribe where/how to find data.\n\nYou must include a link to the original data source(s). Make sure to provide attribution to those who collected the data.\nWhy was the data collected/curated? Who put it together? (This is important, if you don’t know why it was collected then that might not be a good dataset to look at.\n\nDescribe the different data files used and what each variable means.\n\nIf you have many variables then only describe the most relevant ones, possibly grouping together variables that are similar, and summarize the rest.\nUse figures or tables to help explain the data. For example, showing a histogram or bar chart for a particularly important variable can provide a quick overview of the values that variable tends to take.\n\nDescribe any cleaning you had to do for your data.\n\nYou must include a link to your clean_data.R file.\nRename variables and recode factors to make data more clear.\nAlso, describe any additional R packages you used outside of those covered in class.\nDescribe and show code for how you combined multiple data files and any cleaning that was necessary for that.\nSome repetition of what you do in your clean_data.R file is fine and encouraged if it helps explain what you did.\n\nOrganization, clarity, cleanliness of the page\n\nMake sure to remove excessive warnings, use clean easy-to-read code (without side scrolling), organize with sections, use bullets and other organization tools, etc.\nThis page should be self-contained."
  },
  {
    "objectID": "posts/2023-10-13-first-team-meeting/first-team-meeting.html",
    "href": "posts/2023-10-13-first-team-meeting/first-team-meeting.html",
    "title": "First Team Meeting",
    "section": "",
    "text": "These are the steps that you will take today to get started on your project. Today, you will just be brainstorming, and then next week, you’ll get started on the main aspects of the project.\n\nStart by introducing yourselves to each other. I also recommend creating a private channel on Microsoft Teams with all your team members. This will be a place that you can communicate and share ideas, code, problems, etc.\nDiscuss what aspects of the project each of you are more or less excited about. These include\n\nCollecting, cleaning, and munging data ,\nStatistical Modeling,\nVisualization,\nWriting about analyses, and\nManaging and reviewing team work.\n\nBased on this, discuss where you feel your strengths and weaknesses might be.\nNext, start brainstorming questions you hope to answer as part of this project. This question should in some way be addressing issues around racial disparities. The questions you come up with should be at the level of the question we started with when exploring the HMDA data. (“Are there differences in the ease of securing a loan based on the race of the applicant?”) You’ll revise your questions a lot over the course of the project. Come up with a few questions that your group might be interested in exploring.\nBased on these questions, start looking around for data that might help you analyze this. If you are looking at U.S. based data, data.gov is a good source and if you are looking internationally, I recommend checking out the World Bank. Also, try Googling for data. Include “data set” or “dataset” in your query. You might even include “CSV” or some other format. Using “data” by itself in your query often doesn’t work too well. Spend some time searching for data and try to come up with at least three possible data sets. (For your first blog post, you’ll write short proposals about each of them that I’ll give feedback on.)\nCome up with a team name. Next week, I’ll provide the Github Classroom assignment that will be where you work on your final project and you’ll have to have your team name finalized by then. Your project will be hosted online at the website with a URL like sussmanbu.github.io/ma4615-fa23-final-project-TEAMNAME.\n\nNext time, you’ll get your final project website set up and write your first blog post."
  },
  {
    "objectID": "posts/2023-12-20-examples/examples.html",
    "href": "posts/2023-12-20-examples/examples.html",
    "title": "Examples",
    "section": "",
    "text": "Here are some examples of changing the size of a figure.\n\nplot(1:10)\n\n\n\n\n\n\n\n\n\nplot(1:10)\n\n\n\n\n\n\n\n\nWe can also specify column: screen and out-width: 100% so that the figure will fill the screen. plot in the svg vector graphics file format.\n\nlibrary(ggplot2)\nggplot(pressure, aes(x = temperature, y = pressure)) + geom_point()"
  },
  {
    "objectID": "posts/2023-12-20-examples/examples.html#figure-sizing",
    "href": "posts/2023-12-20-examples/examples.html#figure-sizing",
    "title": "Examples",
    "section": "",
    "text": "Here are some examples of changing the size of a figure.\n\nplot(1:10)\n\n\n\n\n\n\n\n\n\nplot(1:10)\n\n\n\n\n\n\n\n\nWe can also specify column: screen and out-width: 100% so that the figure will fill the screen. plot in the svg vector graphics file format.\n\nlibrary(ggplot2)\nggplot(pressure, aes(x = temperature, y = pressure)) + geom_point()"
  },
  {
    "objectID": "posts/2025-03-03-Blog-Post-1/Blog Post 1.html",
    "href": "posts/2025-03-03-Blog-Post-1/Blog Post 1.html",
    "title": "Blog Post 1",
    "section": "",
    "text": "Data Set 1: National Registry of Exonerations (https://www.law.umich.edu/special/exoneration/Pages/detaillist.aspx)\nOne data set we are considering for our project on racial disparities is the National Registry of Exonerations (NRE). This data set (3659 rows, 22 columns) tracks cases of wrongful convictions in the United States and provides detailed information about exonerees, including their race, age, crime type, conviction and exoneration dates, and factors that contributed to their wrongful conviction. The registry, maintained through a collaborative effort involving law schools, innocence organizations, and legal researchers, is designed to shed light on patterns of wrongful convictions and the systemic issues within the justice system. With over 3,000 recorded cases, this data set offers a valuable opportunity to explore how different factors—such as mistaken witness identification, false confessions, or official misconduct—affect exonerees from various racial backgrounds.\nBy analyzing this data, we hope to examine whether racial disparities exist in wrongful convictions and, if so, how they manifest. For instance, do Black exonerees experience official misconduct at a higher rate than white exonerees? Are certain crime types—such as drug offenses or homicides—more likely to lead to wrongful convictions for particular racial groups? Do certain factors—such as official misconduct, inaccurate forensic evidence, or eyewitness misidentification—disproportionately affect exonerees of particular racial groups? Additionally, we want to explore whether exonerees from different racial backgrounds serve significantly different amounts of time before being exonerated. From a practical standpoint, the data comes in a relatively well-structured spreadsheet format (although it is sometimes updated with new cases and additional fields). Loading it into a data analysis environment should be fairly straightforward. However, because this registry is compiled from many sources (court documents, news reports, legal filings, etc.), some challenges with missing or inconsistent data can arise. Despite these limitations, we believe this data set provides a powerful lens through which we can better understand inequities in the criminal justice system and contribute to discussions on how to address them.\nData Set 2: COVID-19 Cases and Deaths by Race/Ethnicity - ARCHIVE (https://data.ct.gov/Health-and-Human-Services/COVID-19-State-Level-Data-Archive/qmgw-5kp6/about_data)\nAnother data set we are considering for our project on racial disparities is the COVID-19 State Level Data - Archive from the Connecticut Department of Public Health (DPH). This dataset tracks state-level COVID-19 metrics and was actively updated until June 1, 2023. It includes 343 rows and 94 columns, offering comprehensive insights into COVID-19 cases, deaths, and testing over time. The data provides cumulative and 7-day rolling metrics, covering confirmed and probable cases, test positivity rates, and reported deaths. The dataset offers an opportunity to investigate racial disparities in COVID-19 outcomes by analyzing statewide trends. Some key questions we hope to explore include: How did COVID-19 case and mortality rates fluctuate over time? Were there significant racial disparities in infection rates and fatalities? How did policy changes, such as testing requirements, impact reported metrics? Additionally, since age-adjusted rates are not directly included, we may need to incorporate external demographic data to better understand disparities.\nFrom a technical perspective, the dataset is well-structured, making it feasible to load into a data analysis environment. However, there are challenges, such as missing data due to delayed reporting and the discontinuation of negative test result tracking after the federal public health emergency ended on May 11, 2023. Despite these limitations, this dataset provides a valuable historical record of state-level COVID-19 impacts and can contribute to broader discussions on health equity and pandemic response policies.\nData Set 3: Drug overdose death rates, by drug type, sex, age, race, and Hispanic origin: United States (https://catalog.data.gov/dataset/drug-overdose-death-rates-by-drug-type-sex-age-race-and-hispanic-origin-united-states-3f72f)\nThe third data set we are considering for our final project is the data on drug overdose death rates, by drug type and selected population characteristics. It includes 15 columns and 6229 rows, which is a large dataset that includes necessary information like gender, race, age to help us have a deeper understand of the trends and patterns surrounding drug overdose fatalities across various demographic groups. By capturing detailed breakdowns by gender, race, and age—along with year-to-year changes—this dataset enables us to pinpoint which subpopulations are most severely affected, how overdose rates have evolved over time, and where public health interventions may be most urgently needed. Some interesting questions we can research on: Are there particular racial or ethnic groups that appear to be disproportionately affected by drug overdoses, and how have these disparities evolved over time? Are there notable interactions between age and gender or age and race in overdose patterns, such as certain age brackets being more susceptible among specific demographic groups?\nFrom a techinical perspective, the dataset is well-organized and contains a rich array of demographic variables, making it feasible to load into most data analysis environments. However, there are challenges, such as potential inconsistencies in data reporting across different regions, limited data coverage for certain demographic groups, and possible variations in drug classification over time. Despite these limitations, this dataset serves as a valuable resource for analyzing patterns and trends in drug overdose fatalities, and it can contribute to broader public health discussions on prevention strategies, resource allocation, and policy-making."
  },
  {
    "objectID": "posts/2025-03-24-Blog-Post-3/BlogPost3.html",
    "href": "posts/2025-03-24-Blog-Post-3/BlogPost3.html",
    "title": "Blog Post 3",
    "section": "",
    "text": "Our dataset is less than 50MB, so we are not putting it in an ignore folder. We are using the entire dataset here (not a subset). We already have created an exoneration_data_clean.rds file that we created last week (generated from our scripts/clean_data.R). The original csv file, titled dataset/publicspreadsheet.csv, is also located in our dataset folder. We would like to note that many columns representing case-related tags contain N/A values (essentially NA is an indicator that this tag is not applied to the case/row): “DNA: Post-conviction DNA testing was conducted and contributed to the exoneration.” “MWID: Mistaken Witness Identification” “FC: False Confession” “P/FA: Perjury or False Accusation” “OM: Official Misconduct” “ILD: Inadequate Legal Defense”"
  },
  {
    "objectID": "posts/2025-03-24-Blog-Post-3/BlogPost3.html#exploratory-data-analysis-of-exoneration-data",
    "href": "posts/2025-03-24-Blog-Post-3/BlogPost3.html#exploratory-data-analysis-of-exoneration-data",
    "title": "Blog Post 3",
    "section": "Exploratory Data Analysis of Exoneration Data",
    "text": "Exploratory Data Analysis of Exoneration Data\nAs part of our data cleaning process, we explored our cleaned dataset to identify unusual values, check for imbalances, and uncover key relationships. Below are some plots and tables that helped us better understand the structure and patterns within the data.\nLoad the cleaned dataset:\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(here)\n\nhere() starts at C:/MA615/ma4615-sp25-final-project-team-10\n\n# Load the cleaned exoneration dataset\nexoneration_data &lt;- read_rds(here::here(\"dataset\", \"exoneration_data_clean.rds\"))\n\n\nMost common crimes in the dataset\n\n\nlibrary(tidyverse)\nlibrary(here)\n\n# Load the cleaned exoneration dataset\nexoneration_data &lt;- read_rds(here::here(\"dataset\", \"exoneration_data_clean.rds\"))\n\nexoneration_data %&gt;%\n  count(Worst_Crime_Display, sort = TRUE) %&gt;%\n  top_n(10) %&gt;%\n  ggplot(aes(x = reorder(Worst_Crime_Display, n), y = n)) +\n  geom_col(fill = \"steelblue\") +\n  coord_flip() +\n  labs(title = \"Top 10 Most Common Crimes Among Exonerees\",\n       x = \"Crime\",\n       y = \"Count\")\n\nSelecting by n\n\n\n\n\n\n\n\n\n\n\nExonerees by Race and Sex\n\n\nexoneration_data %&gt;%\n  count(Race, Sex) %&gt;%\n  ggplot(aes(x = Race, y = n, fill = Sex)) +\n  geom_col(position = \"dodge\") +\n  labs(title = \"Exonerees by Race and Sex\",\n       x = \"Race\",\n       y = \"Number of Exonerees\")\n\n\n\n\n\n\n\n\n\nDistribution of Age at Time of Conviction\n\n\nggplot(exoneration_data, aes(x = Age)) +\n  geom_histogram(binwidth = 5, fill = \"tomato\", color = \"white\") +\n  labs(title = \"Distribution of Exonerees' Ages at Conviction\",\n       x = \"Age\",\n       y = \"Count\") +\n  theme_minimal()\n\nWarning: Removed 27 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\n\n\n\n\n\n\n\n\nTrends in Exoneration Over Time\n\n\nexoneration_data %&gt;%\n  count(Exonerated) %&gt;%\n  ggplot(aes(x = Exonerated, y = n)) +\n  geom_line(group = 1, color = \"darkgreen\") +\n  geom_point() +\n  labs(title = \"Number of Exonerations Over Time\",\n       x = \"Year of Exoneration\",\n       y = \"Count\")\n\n\n\n\n\n\n\n\n\nTable: DNA Evidence by Race\n\n\nexoneration_data %&gt;%\n  filter(!is.na(DNA)) %&gt;%\n  count(Race, DNA) %&gt;%\n  pivot_wider(names_from = DNA, values_from = n, values_fill = 0)\n\n# A tibble: 6 × 2\n  Race              DNA\n  &lt;chr&gt;           &lt;int&gt;\n1 Asian               1\n2 Black             342\n3 Hispanic           58\n4 Native American     4\n5 Other               3\n6 White             206"
  },
  {
    "objectID": "posts/2025-04-07-Blog-Post-5/BlogPost5.html",
    "href": "posts/2025-04-07-Blog-Post-5/BlogPost5.html",
    "title": "Blog Post 5",
    "section": "",
    "text": "Combining Two Datasets for Deeper Insights\nOur initial analysis centered on the National Registry of Exonerations dataset, which captures cases where wrongfully convicted individuals were later exonerated. It gave us a unique perspective on how wrongful convictions occur and the factors—such as official misconduct or false confessions—that contribute to these errors. However, we are going to include at least one additional dataset to enrich our analysis. This week, we turned our attention to publicly available records of general crime data in Chicago, aiming to compare exoneration trends to the broader pool of convictions and reported crimes.\nSelecting the Chicago Crime Dataset\nWe chose to work with the “Crimes – 2001 to Present” dataset from the City of Chicago data portal. This dataset offers a comprehensive record of crimes reported in Chicago, including details such as the date of offense, type of crime, and location information. Beyond providing a large sample of data from a single metropolitan area, it directly complements our exoneration dataset: while our exoneration data focuses on cases where convictions were overturned, the Chicago crime data shows us the larger context of how many people are arrested, charged, and convicted of various offenses over time.\nCombining the Data\nSince our exoneration data covers the entire United States, we first will filter it to focus on Cook County, Illinois, where Chicago is located. We specifically chose Cook County because it stands out in the exonerations dataset, displaying one of the highest numbers of exonerations nationwide. This makes Cook County a particularly rich location for studying both wrongful convictions and overall crime trends (see graph below).\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(ggplot2)\n# Load the cleaned exoneration dataset\nexoneration_data &lt;- readRDS(here::here(\"dataset\", \"exoneration_data_clean.rds\"))\n\nlibrary(dplyr)\nlibrary(ggplot2)\n\n# Count exonerations by county and sort\ntop_counties &lt;- exoneration_data %&gt;%\n  filter(!is.na(County)) %&gt;%\n  group_by(State, County) %&gt;%\n  summarise(Exoneration_Count = n(), .groups = \"drop\") %&gt;%\n  arrange(desc(Exoneration_Count)) %&gt;%\n  slice_head(n = 10) %&gt;%\n  mutate(County_Label = paste(County, \"(\", State, \")\", sep = \" \"))\n\n# Plot\nggplot(top_counties, aes(x = reorder(County_Label, Exoneration_Count), y = Exoneration_Count)) +\n  geom_col(fill = \"darkred\") +\n  coord_flip() +\n  labs(\n    title = \"Top 10 Counties with the Most Exonerations\",\n    x = \"County (State)\",\n    y = \"Number of Exonerations\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nOur initial plan is to align the two datasets primarily by date. We want to observe how exonerations in Cook County track alongside the volume and nature of crimes reported in Chicago over the same time span. For instance, we might compare annual trends: in years where Chicago saw higher rates of certain crimes (like homicide or assault), did we also see higher numbers of exonerations for those crimes later on?\nIn practice, combining the data means creating a common time structure—in this case, year or month-year—and then grouping or aggregating metrics from each dataset. In the exonerations data, we have case-level information (e.g., date of conviction or exoneration), while the Chicago crime data can be grouped by date reported or arrest date. We can then connect these by matching year (and possibly month) to compare how many crimes were committed versus how many exonerations occurred in that same window.\nInitial Findings and Challenges\nAlthough we are still in the early stages of combining the two datasets, we have already uncovered a few insights and challenges. First, Cook County’s exoneration cases span a wide range of years, including cases that started well before 2001 (the earliest date in our Chicago crime dataset). As a result, a direct one-to-one match for all exonerations is not possible. We’ll need to focus on exonerations that occurred or that began in the timeframe where Chicago crime data is available, and focus on trends from the big picture.\nSecond, the Chicago crime dataset is very large. It contains millions of rows of data since 2001, which can present performance challenges. We plan to filter it more narrowly, focusing on the worst crimes in our exoneration dataset—like homicide—so we can draw cleaner comparisons and avoid memory issues or very long processing times.\nLastly, we noted that our exoneration dataset tracks a different “lifecycle” of a criminal case. An exoneration might occur many years after the original conviction date. Meanwhile, the Chicago crime data captures the incident date and basic details of the offense but does not necessarily track outcome in the courts. Thus, comparing “crimes filed” to “exonerations” is more complex than a simple 1:1 join. We are approaching this by looking at broad trends rather than trying to match individual cases.\nNext Steps\nOver the coming week, we will refine our data filtering strategies and aggregation methods. We will likely create an annual-level panel that shows, for each year, the number of convictions (as recorded by arrests or charges in the Chicago crime data) versus the number of exonerations in Cook County for major crime categories. We then plan to explore:\n\nWhether spikes in reported crimes correlate with spikes in wrongful convictions a few years later.\nThe types of crimes most commonly associated with exonerations in Cook County, and how those compare to the overall crime landscape in Chicago.\n\nBy integrating these two datasets, we hope to provide a richer picture of how wrongful convictions fit into the broader criminal justice context in Chicago, and potentially highlight systemic issues that contribute to both wrongful convictions and crime trends more generally."
  },
  {
    "objectID": "posts/2025-04-22-Blog-Post-7/BlogPost7.html",
    "href": "posts/2025-04-22-Blog-Post-7/BlogPost7.html",
    "title": "BlogPost 7",
    "section": "",
    "text": "Designing Our Interactive Dashboard: Exploring Justice in Cook County\nAs we approach the final stretch of our project, we’ve started planning the interactive dashboard that will serve as the centerpiece of our analysis. Our goal is to build an interface that brings together two rich datasets—exonerations across the U.S. and crimes committed in Chicago since 2001—to explore systemic patterns of justice and injustice in Cook County. This dashboard won’t just display statistics—it will give users the ability to interact with and investigate the data on their own terms.\nCore Vision and Features\nAt the heart of our interactive will be the ability to see big picture trends—like how exonerations in Cook County have changed over time—and to zoom in on more granular comparisons, such as how often exonerations follow convictions for specific types of crimes. To achieve this, our dashboard will include:\n\nA timeline view showing the number of exonerations and total crimes over time (e.g., by year or month), allowing users to explore trends and compare timelines side by side.\nFilter options to narrow the dataset by crime type (e.g., homicide, drug offenses), race of the exoneree, and whether key tags like “DNA evidence” or “official misconduct” were involved.\nCounty focus: Since Cook County dominates the exoneration dataset and is the sole location in the crime dataset, this geographic focus lets users meaningfully explore the relationship between the two datasets in one place.\n\nWe also plan to implement clickable visual elements, such as a bar chart of the most common crimes that lead to exoneration. When a user clicks on a crime type (e.g., sexual assault), the entire dashboard would update to show:\n\nThe total number of crimes of that type reported in Chicago since 2010,\nThe number of exonerations tied to those crimes,\nAnd characteristics of those exoneration cases (race, age, evidence types, etc.)\n\nInteractivity for Exploration\nTo make the interactive feel interactive—not just visually dynamic—we’ll let users control their own exploration. This includes dropdowns, checkboxes, and sliders that filter the data and update charts in real time. Some specific ideas include:\n\nA year range slider to focus on certain time periods (e.g., post-2015),\nA race selector to explore how trends differ across racial groups,\nA checkbox panel for key exoneration tags like false confessions, perjury, or mistaken witness ID.\n\nThese features give users the tools to ask their own questions: Have exonerations increased in years when certain types of crimes spiked? Do some evidence types appear more often for Black exonerees than White exonerees?\nPersonal and Guided Experience\nWhile not all dashboards lend themselves to personalization, we see potential here. For example, users could filter the data by their own neighborhood (using ZIP code or community area) if we incorporate geolocation info from the crime data. This would allow a visitor from Chicago to explore: How many crimes have been reported near me? Are any exonerations tied to my area? This local perspective could help personalize the broader systemic patterns we’re analyzing.\nWe also plan to guide users through the dashboard with annotations and highlighted callouts—for example, pointing out that Cook County alone accounts for over X% of national exonerations, or flagging years with particularly high exoneration counts. These will serve as jumping-off points for users who don’t know where to start their exploration.\nCurrent Progress\nSo far, we’ve completed cleaning both datasets and written them to .rds files for quick loading. We’ve filtered the Chicago crime data to only include cases since 2010, and we’ve cleaned the exoneration data to allow easy filtering by county, crime type, and evidence tags. We have been analyzing patterns and trying to decide on our main question that we would like our dashboard to address. Our next steps are to build the visual components using shiny, plotly, or flexdashboard, and to link them together with reactive inputs.\nBy bringing together both datasets in an accessible and engaging format, we hope to help users better understand not just where injustice happens—but how it happens, who it affects, and what patterns lie beneath the surface."
  }
]